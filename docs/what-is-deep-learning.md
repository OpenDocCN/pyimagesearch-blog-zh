# 什么是深度学习？

> 原文：<https://pyimagesearch.com/2021/04/17/what-is-deep-learning/>

> *深度学习方法是具有多级表示的表示学习方法，通过组合简单但非线性的模块获得，每个模块将一级(从原始输入开始)的表示转换为更高、稍微更抽象级别的表示。[.。。深度学习的关键在于，这些层不是由人类工程师设计的:它们是使用通用学习程序从数据中学习的。*
> 
> *—* [Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, Nature (2015), p. 436](https://doi.org/10.1038/nature14539)

深度学习是机器学习的一个子领域，而机器学习又是人工智能(AI)的一个子领域。有关这种关系的图示，请参考**图 1** 。

人工智能的核心目标是提供一套算法和技术，可以用来解决人类直观地执行*和近乎自动地执行*的问题，但在其他方面对计算机来说非常具有挑战性。这类人工智能问题的一个很好的例子是解释和理解图像的内容——这项任务人类可以不费吹灰之力完成，但事实证明机器完成这项任务*极其困难*。**

 **而人工智能包含了大量不同的与自动机器推理相关的工作(推理、计划、启发等)。)，机器学习子领域往往是*对**模式识别**和**从数据中学习**特别感兴趣*。

人工神经网络(ann)是一类机器学习算法，从数据中学习，专门从事模式识别，受大脑结构和功能的启发。正如我们将发现的那样，深度学习属于 ANN 算法家族，在大多数情况下，这两个术语可以互换使用。事实上，你可能会惊讶地发现，深度学习领域已经存在了超过 60 年*，根据研究趋势、可用的硬件和数据集以及当时杰出研究人员的流行选择，它有不同的名称和化身。*

 *在本章的剩余部分，我们将回顾深度学习的简史，讨论什么使神经网络“深度”，并发现“分层学习”的概念，以及它如何使深度学习成为现代机器学习和计算机视觉的主要成功故事之一。

## **神经网络和深度学习的简明历史**

神经网络和深度学习的历史很长，有点令人困惑。你可能会惊讶地知道，“深度学习”自 20 世纪 40 年代以来就存在了，经历了各种名称的变化，包括*控制论*、*连接主义*，以及最熟悉的*人工神经网络* (ANNs)。

虽然*受到人类大脑及其神经元如何相互作用的启发，但人工神经网络*不是*大脑的现实模型。相反，它们是一种灵感，让我们能够在一个非常基本的大脑模型和我们如何通过人工神经网络模仿这种行为之间进行比较。*

第一个神经网络模型来自于 1943 年 [**麦卡洛克和**皮茨](http://dl.acm.org/citation.cfm?id=65669.104377)。这个网络是一个*二元分类器*，能够根据一些输入识别两个不同的类别。问题是，用于确定给定输入的类别标签的*权重*需要由人工*手动调整*——如果需要人工操作员干预，这种类型的模型显然不能很好地扩展。

然后，在 20 世纪 50 年代，开创性的感知器算法由**Rosenblatt(**[**1958**](https://doi.org/10.1037/h0042519)**，**[1962](https://babel.hathitrust.org/cgi/pt?id=mdp.39015039846566&view=1up&seq=1)**)**发表——这个模型可以*自动*学习对输入进行分类所需的权重(不需要人工干预)。感知器架构的一个例子可以在图 2 中看到。事实上，这种自动训练程序形成了随机梯度下降法(SGD)的基础，该方法至今仍用于训练*深度*神经网络。

在此期间，基于感知机的技术在神经网络社区中风靡一时。然而，Minsky 和 Papert 在 1969 年发表的一篇 [**实际上使神经网络研究停滞了近十年。他们的工作表明，具有线性激活函数(不考虑深度)的感知器仅仅是一个线性分类器，无法解决非线性问题。非线性问题的典型例子是**图 3** 中的 XOR 数据集。现在花一点时间说服自己，尝试用一条单线把蓝色星星和红色圆圈分开是不可能的。**](https://mitpress.mit.edu/books/perceptrons)

此外，作者认为(当时)我们没有构建大型深度神经网络所需的计算资源(事后看来，他们是绝对正确的)。光是这一篇论文*就差点要了*神经网络研究的命。

幸运的是，反向传播算法和[**Werbos(1974)**](https://dokumen.pub/beyond-regression-new-tools-for-prediction-and-analysis-in-the-behavioral-sciences.html)[**Rumelhart 等人(1986)**](http://dl.acm.org/citation.cfm?id=%2065669.104451) 和 [**LeCun 等人(1998)**](http://dl.acm.org/citation.cfm?id=645754.668382) 的研究能够使可能已经早期消亡的神经网络复苏。他们对反向传播算法的研究使得*多层前馈*神经网络能够被训练(**图 4** )。

结合非线性激活函数，研究人员现在可以学习非线性函数并解决 XOR 问题，从而打开了神经网络研究的全新领域。进一步的研究表明，神经网络是 [*通用逼近器*](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&rep=rep1&type=pdf) ，能够逼近任何连续函数(但不能保证网络是否真的能够*学习*表示函数所需的参数)。

反向传播算法是现代神经网络的基石，允许我们有效地训练神经网络，并“教”它们从错误中学习。但即使如此，在这个时候，由于(1)缓慢的计算机(与现代机器相比)和(2)缺乏大的、有标签的训练集，研究人员无法(可靠地)训练具有两个以上隐藏层的神经网络——这在计算上是不可行的。

今天，我们所知的神经网络的最新化身被称为**深度学习**。将深度学习与其之前的化身区分开来的是，我们拥有更快、更专业的硬件，以及更多可用的训练数据。我们现在可以用*训练更多隐藏层*的网络，这些隐藏层能够进行分层学习，在较低层学习简单的概念，在网络的较高层学习更抽象的模式。

也许将深度学习应用于特征学习的典型例子是应用于手写字符识别的 [*卷积神经网络* (LeCun 等人，1998)](https://ieeexplore.ieee.org/document/726791)*自动*从图像中学习区分模式(称为“过滤器”)，方法是依次将层堆叠在彼此之上。网络中较低级别的过滤器表示边缘和角点，而较高级别的图层使用边缘和角点来学习更抽象的概念，这些概念对于区分图像类别非常有用。

在许多应用中，CNN 现在被认为是最强大的图像分类器，目前负责推动利用机器学习的计算机视觉子领域的最新发展。要更彻底地回顾神经网络和深度学习的历史，请参考 [**Goodfellow 等人(2016)**](http://www.deeplearningbook.org) 以及 [**Jason Brownlee (2016)在机器学习掌握**](http://machinelearningmastery.com/what-is-deep-learning/) 发表的这篇优秀博文。

## **分层特征学习**

机器学习算法(一般)分为三个阵营——*监督*、*非监督*和*半监督*学习。我们将在本章讨论监督和非监督学习，而半监督学习留待将来讨论。

在受监督的情况下，机器学习算法被给予一组*输入*和*目标* *输出*。然后，该算法尝试学习可用于将输入数据点自动映射到其正确目标输出的模式。监督学习类似于让老师看着你考试。鉴于你以前的知识，你尽最大努力在你的考试上标出正确答案；然而，如果你是不正确的，你的老师下次会引导你做出更好的、更有教育意义的猜测。

在无人监督的情况下，机器学习算法试图自动发现有区别的特征*，而不需要*任何关于输入是什么的提示。在这种情况下，我们的学生试图将相似的问题和答案组合在一起，即使学生不知道正确答案是什么*和*老师不在那里为他们提供正确的答案。与监督学习相比，无监督学习显然是一个更具挑战性的问题——通过知道答案(即目标输出)，我们可以更容易地定义可以将输入数据映射到正确目标分类的判别模式。

在应用于图像分类的机器学习的背景下，机器学习算法的目标是获取这些图像集，并识别可用于将各种图像类别/对象彼此区分开的模式。

过去，我们使用*手工设计的特征*来量化图像的内容——我们*很少*使用原始像素强度作为我们机器学习模型的输入，这在深度学习中很常见。对于我们数据集中的每张图像，我们都执行了*特征提取*，或获取输入图像，根据某种算法(称为*特征提取器*或*图像描述符*)对其进行量化，并返回旨在量化图像内容的向量(即一系列数字)的过程。**图 5** 描绘了通过一系列黑盒颜色、纹理和形状图像描述符来量化包含处方药丸药物的图像的过程。

我们手工设计的特征试图编码纹理([](https://ieeexplore.ieee.org/document/1017623)****、** [**哈拉里克纹理**](http://dx.doi.org/10.1109/tsmc.1973.4309314) )、形状( [**胡矩**](https://ieeexplore.ieee.org/document/1057692) 、 [**泽尼克矩**](http://dx.doi.org/10.1109/34.55109) )和颜色( [**颜色矩、颜色直方图、颜色相关图**](http://dl.acm.org/citation.cfm?id=794189.794514) )。**

 **其他方法如关键点检测器([、、FAST 、、](http://dx.doi.org/%2010.1109/ICCV.2005.104)[、 **Harris** 、、](https://www.semanticscholar.org/paper/A-Combined-Corner-and-Edge-Detector-Harris-Stephens/6818668fb895d95861a2eb9673ddc3a41e27b3b3)[、 DoG 、等)和局部不变描述符(](http://dl.acm.org/citation.cfm?id=850924.851523)[、 SIFT 、、](http://dl.acm.org/citation.cfm?id=850924.851523)[、 SURF 、T19、](http://dx.doi.org/10.1016/j.cviu.2007.09.014)[、T21【BRIEF】、、](http://dl.acm.org/citation.cfm?id=1888089.1888148)[、 ORB 、等。)描述图像的*显著*(即最“有趣”)区域。](http://dx.doi.org/10.1109/ICCV.2011.6126544)

其他方法如 [**梯度方向直方图(HOG)**](http://dx.doi.org/10.1109/CVPR.2005.177) 被证明在检测图像中的对象时非常好，当我们的图像的视角与我们的分类器被训练的视角没有显著变化时。使用 HOG +线性 SVM 检测器方法的一个例子可以在**图 6** 中看到，我们在图像中检测停车标志的存在。

有一段时间，图像中对象检测的研究是由 HOG 及其变体指导的，包括计算昂贵的方法，如 [**可变形部分模型**](http://dx.doi.org/10.1109/TPAMI.2009.167) 和 [**样本支持向量机**](https://ieeexplore.ieee.org/document/6126229) 。

在每一种情况下，算法都是由*手工定义的*来量化和编码图像的特定方面(即形状、纹理、颜色等)。).给定像素的输入图像，我们将对像素应用我们手动定义的算法，并作为回报接收量化图像内容的特征向量——图像像素本身除了作为我们特征提取过程的输入之外，没有其他用途。从特征提取中得到的特征向量是我们真正感兴趣的，因为它们是我们机器学习模型的输入。

深度学习，特别是卷积神经网络，采取了不同的方法。不是手动定义一组规则和算法来从图像中提取特征，**而是从训练过程中自动学习这些特征**。

再次，让我们回到机器学习的目标:*计算机应该能够从* *的经验(即例子)中学习它们试图解决的问题*。

利用深度学习，我们试图从概念的层次来理解问题。每个概念都建立在其他概念的基础上。网络低层中的概念对问题的一些基本表示进行编码，而高层*使用这些基本层*来形成更抽象的概念。这种分层学习允许我们*完全去除*手工设计的特征提取过程，并将 CNN 视为端到端学习器。

给定一幅图像，我们将像素亮度值作为**输入**提供给 CNN。一系列的**隐藏层**被用来从我们的输入图像中提取特征。这些隐藏层以分层的方式建立在彼此之上。首先，在网络的较低层中只检测到类似边缘的区域。这些边缘区域用于定义拐角(边缘相交的地方)和轮廓(对象的轮廓)。组合角和轮廓可以导致下一层中的抽象“对象部分”。

再次请记住，这些过滤器正在学习检测的概念类型是*自动* *人工学习的—* 在学习过程中没有我们的干预。最后，**输出**层用于对图像进行分类，并获得输出类别标签——输出层或者直接受到网络中每个其他节点的*影响*或者间接受到网络中每个其他节点的*影响。*

我们可以将这一过程视为分层学习:网络中的每一层都使用前几层的输出作为“构建模块”，来构建越来越抽象的概念。这些层是自动学习的*—*—*在我们的网络中没有手工制作的特征工程*。**图 7** 将使用手工制作特征的经典图像分类算法与通过深度学习和卷积神经网络的表示学习进行了比较。

深度学习和卷积神经网络的主要好处之一是，它允许我们跳过特征提取步骤，而是专注于训练我们的网络来学习这些过滤器的过程。然而，正如我们将在本书后面发现的，训练网络以在给定的图像数据集上获得合理的精度并不总是一件容易的事情。

## ****“深”有多深？****

引用 Jeff Dean 2016 年的演讲*中的 [**深度学习构建智能计算机系统**](http://static.googleusercontent.com/media/research.google.com/en//people/jeff/BayLearn2015.pdf)* :

> 当你听到深度学习这个术语时，只需想到一个大型的深度神经网络。深度指的是典型的层数，这是一个流行的术语，已经被媒体采用。

这是一个很好的引用，因为它允许我们将深度学习概念化为大型神经网络，其中各层建立在彼此之上，逐渐增加深度。问题是我们仍然没有一个具体的答案，“一个神经网络需要多少层才能被认为是 ***深？”***

简而言之，专家们对一个网络的深度没有达成共识****(**[**good fellow 等人，2016**](http://www.deeplearningbook.org) **)** 。**

 **现在我们需要看看网络类型的问题。根据定义，卷积神经网络(CNN)是一种深度学习算法。但假设我们有一个只有一个卷积层的 CNN 是一个浅层的网络，但仍然属于深度学习阵营中被认为是“深度”的算法家族？

我个人的观点是，任何大于两个隐藏层的网络都可以被认为是“深度的”我的推理是基于以前在人工神经网络中的研究，这些人工神经网络存在以下严重缺陷:

1.  我们缺乏可用于训练的大规模、带标签的数据集
2.  我们的计算机速度太慢，无法训练大型神经网络
3.  激活功能不足

由于这些问题，在 20 世纪 80 年代和 90 年代(当然，还有更早的时期)，我们不容易训练具有两个以上隐藏层的网络。事实上， [**Geoff Hinton 在他 2016 年的演讲**](https://www.axios.com/artificial-intelligence-pioneer-says-we-need-to-start-over-1513305524-f619efbd-9db0-4947-a9b2-7a4c310a28fe.html)*中支持这种观点，他在演讲中讨论了为什么深度学习的前身在 20 世纪 90 年代没有起飞:*

 *1.  我们标记的数据集小了几千倍。
2.  我们的计算机慢了几百万倍。
3.  我们用一种愚蠢的方式初始化了网络权重。
4.  我们使用了错误类型的非线性激活函数。

所有这些原因都指向一个事实，即训练深度超过两个隐藏层的网络是徒劳的，如果不是计算上的，也是不可能的。

在当前的化身中，我们可以看到潮汐已经改变。我们现在有:

1.  更快的计算机
2.  高度优化的硬件(即 GPU)
3.  数百万张图像数量级的大型标记数据集
4.  更好地理解重量初始化功能以及哪些功能起作用/不起作用
5.  优越的激活函数和理解为什么以前的非线性函数停滞不前的研究

套用[吴恩达 2013 年的演讲， ***深度学习，自学和无监督特征学习***](https://www.youtube.com/watch?v=n1ViNeWhC24) ，我们现在能够构建更深层次的神经网络，并用更多的数据训练它们。

随着网络的*深度*增加，*分类精度*也增加。这种行为不同于传统的机器学习算法(即逻辑回归、支持向量机、决策树等。)，在这种情况下，即使可用的训练数据增加，我们的性能也会达到一个稳定的水平。受 [**吴恩达 2015 年演讲*启发的一个情节，关于深度学习***](https://www.slideshare.net/ExtractConf) 科学家应该知道的数据，可以在**图 8** 中看到，提供了这种行为的一个例子。

随着训练数据量的增加，我们的神经网络算法获得了更高的分类精度，而以前的方法在某一点上停滞不前。由于更高的准确性和更多数据之间的关系，我们倾向于将*深度学习*与*大型数据集*联系起来。

在开发自己的深度学习应用程序时，我建议使用以下经验法则来确定您给定的神经网络是否是深度的:

1.  您是否正在使用一种专门的网络架构，如卷积神经网络、循环神经网络或长短期记忆(LSTM)网络？如果是这样，**是的，你正在执行深度学习。**
2.  你的网络有深度 *>* 2 吗？如果是，**你在做深度学习**。
3.  你的网络有深度 *>* 10 吗？如果是这样，**你正在执行*非常*** [**深度学习**](http://arxiv.org/abs/1404.7828) 。

尽管如此，尽量不要陷入深度学习和什么是/不是深度学习的流行词汇中。最核心的是，在过去的 60 年里，深度学习已经经历了许多不同的化身，基于各种思想流派——**但*这些思想流派中的每一个*都围绕着受大脑结构和功能启发的人工神经网络**。不管网络深度、宽度或专门的网络架构如何，你仍然*使用人工神经网络*执行机器学习。

## **总结**

这一章解决了复杂的问题*“什么是深度学习？”*

正如我们发现的那样，深度学习自 20 世纪 40 年代以来就已经存在，根据不同的思想流派和特定时间的流行研究趋势，它有不同的名称和化身。在最核心的地方，深度学习属于人工神经网络(ann)家族，这是一套学习受大脑结构和功能启发的模式的算法。

专家们对于究竟是什么让神经网络变得“深”没有达成一致意见；然而，我们知道:

1.  深度学习算法以分层的方式学习，因此将多个层堆叠在彼此之上，以学习越来越多的抽象概念。
2.  一个网络要有 *>* 2 层才算“深”(这是我基于几十年神经网络研究的坊间看法)。
3.  具有 *>* 10 层的网络被认为是*非常深*(尽管这个数字会随着诸如 ResNet 之类的架构被成功训练超过 100 层而改变)。

如果你在阅读本章后感到有点困惑甚至不知所措，不要担心——这里的目的只是提供深度学习的一个非常高层次的概述以及“深度”的确切含义。

本章还介绍了一些你可能不熟悉的概念和术语，包括像素、边缘和拐角——我们的下一章将讨论这些类型的图像基础知识，并给你一个坚实的基础。然后，我们将开始进入神经网络的基础，使我们能够在本书的稍后部分过渡到深度学习和卷积神经网络。虽然这一章是公认的高级别，但本书的其余章节将非常实用，允许您掌握计算机视觉概念的深度学习。********